% SPDX-FileCopyrightText: 2023 Jean-Sebastian de Wet, Jan-Philipp Kiel, Pascal Mager
% SPDX-License-Identifier: CC-BY-4.0
%
% LLNCS macro package for Springer Computer Science proceedings Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
%
\usepackage{listings}
% Used to display Python code.
%
\begin{document}
%
\title{Property-Based Testing of Data Analysis Scripts}
\subtitle{A Focus on Hypothesis for Python}
%\subtitle{Enhancing Reliability in DLR Research}  % Alternative subtitle
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Jean-Sebastian de Wet \and
  Jan-Philipp Kiel \and
  Pascal Mager}
%
\authorrunning{Group Criterion}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{University of Cologne, Cologne, Germany}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
  This paper explores property-based testing as a method to ensure data analysis scripts' reliability, especially in DLR research using Python, Pandas, and Matplotlib. It outlines challenges with traditional testing in scenarios with diverse data values, emphasizing the need for innovative testing strategies. The paper thoroughly covers property-based testing, including its history, key principles, use cases, and integration in the test pyramid. It then focuses on Hypothesis for Python, a powerful tool for property-based testing, discussing its use, integration with pytest, and unique features. Real-world application is demonstrated with code examples, highlighting how property-based testing, especially with Hypothesis, strengthens data analysis scripts' reliability. The paper concludes by summarizing key findings and emphasizing the crucial role property-based testing, like Hypothesis, plays in boosting researchers' confidence with unknown data.

  \keywords{Property-Based Testing \and Data Analysis Scripts \and Hypothesis \and Python \and pytest \and Reliability \and Test Pyramid \and Code Examples \and DLR Research.}
\end{abstract}
%
%
%
% [Page 1]
\section{Introduction}
In the evolving landscape of data analysis, the complexity and volume of datasets have grown exponentially~\cite{Taylor2023}, presenting unique challenges across various fields. This surge in data complexity necessitates robust testing methodologies to ensure the accuracy and reliability of data analysis tools and scripts. Traditional testing approaches, primarily based on specific input-output cases, often fall short in addressing the dynamic and unpredictable nature of modern datasets. These limitations are particularly evident in specialized fields like aerospace research, where the data's scope and diversity are exceptionally vast.

% [Page 1-2]
\section{Background}
Within the German Aerospace Center (DLR), the reliability of data analysis scripts is a cornerstone of successful research outcomes. DLR researchers frequently use Python~\cite{Kurnatowski2020}, along with its powerful libraries like Pandas\footnote{https://pandas.pydata.org/, accessed: 21.01.2024} and Matplotlib\footnote{https://matplotlib.org/, accessed: 21.01.2024}, for complex data manipulations and visualizations. This introduces significant testing challenges. DLR, being at the forefront of aerospace research and development, deals with an enormous range of data variables, from satellite imagery to flight dynamics.\footnote{https://www.dlr.de/en/dlr/about-us, accessed: 21.01.2024} This variety and complexity of data make testing particularly challenging. This paper explores the adoption of property-based testing, presenting it as an innovative and essential strategy to overcome these testing challenges, especially in scenarios involving large possible value ranges of data.

\section{Method}
Our methodology in tackling these challenges involved two key phases.

\subsection{Literature Study}
The first phase of our methodology involved a thorough literature review on property-based testing. Our objective was to gain an in-depth understanding of its theoretical foundations, including its history, key principles, and diverse applications. The process entailed systematic searches in academic databases and online repositories, using targeted keywords such as "property-based testing" and "automatic test case generation." We focused on selecting literature that emphasized the capability of property-based testing in generating a wide range of test cases, an essential feature for managing complex datasets typical in DLR research.

\subsection{Prototyping}
The second phase centered on practical application, where we developed a comprehensive guide for using Hypothesis, a property-based testing framework for Python. This guide covered critical aspects including installation, configuration, and integration with Python's pytest\footnote{https://pytest.org/, accessed: 21.01.2024} framework. It also provided practical examples demonstrating the process of defining properties and generating test cases using Hypothesis. To validate our methodology, we conducted a case study that involved applying property-based testing to a data analysis script related to astronauts. This case study served as a real-world application, illustrating the practicality of property-based testing in complex data scenarios.

\section{Results}
First, we uncovered key theoretical insights on property-based testing through our literature study. Then, we demonstrated the practical effectiveness of this approach in our prototype using Hypothesis.

\subsection{Overview of Property-Based Testing}
\subsubsection{History of Property-Based Testing}
The origins of property-based testing (PBT) can be traced back more than 20 years ago, even before 2000. Although it had already been a topic within information technology research like in Goldreich (1998) and Fink (1997), it gained much more attention through the development of QuickCheck\footnote{https://www.cse.chalmers.se/~rjmh/QuickCheck/, accessed: 21.01.2024}~\cite{Shi2023,Fink1997,MacIver2019,Honarvar2020}. Beginning with research questions concerning topics like automation of test input generation and automated techniques in general~\cite{Fink1997} and guiding the automated input generator towards values with higher probability of failure~\cite{Loescher2017}, more recent papers deal with the implementation of different frameworks or platforms and techniques of PBT-application~\cite{Padhye2019,Honarvar2020,Shi2023,Corgozinho2023}. To top it off, PBT already enjoys wide-ranging support in different programming languages including automation capabilities~\cite{Chen2022,Padhye2019,Honarvar2020,ElazarMittelman2023,Shi2023}, as well as the application within many different python projects~\cite{Corgozinho2023}.

\subsubsection{How Property-Based Testing Works}
PBT is a method enabling the formal verification of a software~\cite{Chen2022,Fink1997,Honarvar2020,Paraskevopoulou2015}, with the key concept of validating high-level or general properties of a software~\cite{Fink1997,Loescher2017,Honarvar2020,Corgozinho2023}. Test cases used within the application of this method, are usually formulated using logical descriptions of a software's expected behaviour~\cite{Chen2022,Fink1997,Honarvar2020,Loescher2017,Corgozinho2023}. More explicitly, these tests may include pre- or post-conditions of the system~\cite{Honarvar2020}. In order to provide formal validation of the system's behaviour, a single test case is executed many times with randomly generated input in search for counterexamples resulting in violation of a specific property or even a crash of a software, therefore invalidating said property~\cite{Chen2022,Loescher2017,Padhye2019,ElazarMittelman2023,Paraskevopoulou2015,Corgozinho2023}. For randomly generating input, data generators are used~\cite{Chen2022,Loescher2017,Padhye2019,ElazarMittelman2023} which can be adjusted according to "domain-specific knowledge"~\cite{Chen2022}. Through this automated execution of tests with random input PBT tries to approximate the validity of a certain property, as it has to withstand the check using many different instantiations within a given input range or otherwise the property is falsified~\cite{Fink1997,ElazarMittelman2023,Corgozinho2023,Paraskevopoulou2015}. To further elaborate on the properties, which represent the desired behaviour in terms of input / output of given tested functions through specifications~\cite{Chen2022,Fink1997,Loescher2017}, some examples can be given. To start with a simple one, think of a function that adds two numbers (A, B) and returns the sum of both numbers (A + B). Hier ggfs. Beispiel für sortierter Liste einfügen --> Beinhaltet "Invariante" You can define a test which asserts that for any given input for either A and B, the function will return the addition of both numbers~\cite{Corgozinho2023}. Another example which is frequently used for showcasing PBT are binary search trees~\cite{Corgozinho2023,Shi2023}. In PBT "one desirable property [could be], that if we [you] insert a key into a valid BST, then it should remain a valid BST"~\cite{Shi2023}. In the context of PBT you would then use logic expressions for your tests and use a random input generator to check whether the given property is violated. Other possible use cases could be software-security related as example in the case of authentication~\cite{Fink1997} or "the correctness of hardware [and] the external software involved"~\cite{Chen2022}. Long story short, PBT allows for the formal verification of a system's invariants~\cite{Fink1997,ElazarMittelman2023,Corgozinho2023}.

\subsubsection{Advantages and Disadvantages of Property-Based Testing}
As already mentioned, PBT allows to formally validate the correctness of a software by testing specified properties using randomly generated input for each test. However, describing all of a system's expected behaviour in a logical style is paired with reduced feasibility~\cite{Chen2022,Koopman2012}. By applying PBT, required endeavour for formal validation can be lowered~\cite{Hritcu2016,Chen2022,Paraskevopoulou2015}. Moreover, specifications used for PBT might also improve cooperation between software engineers (SE) and software testers in larger projects, as the language used for defining tests is easier to grasp compared to abstract proofs~\cite{Chen2022,Loescher2017}. Besides this, PBT can also be applied to test in "multiple domains"~\cite{Karlsson2019} - to name a few examples next to the given examples of the previous chapter: interfaces~\cite{Karlsson2019,Francisco2013,LamelaSeijas2013}, e.g. by testing invariants regarding the responses of requested URLs of REST-APIs~\cite{Karlsson2019}. Other possible domains are telecom systems~\cite{Arts2006}, file synchronisation services~\cite{Hughes2016} and databases~\cite{Arts2015}. Despite its wide applicability and advantages regarding formal validation, it lowers engineering effort in terms of defining individual test cases as well as input parameters~\cite{Chen2022,Loescher2017,Corgozinho2023} and might as well offer incentives for SEs to design the code to be easily expressed by properties~\cite{Chen2022}. More specifically, when compared to manually written tests like in unit-testing, PBT allows the SE to put more emphasis on ensuring and restoring correctness of the software and less on "defining test case inputs, examples, and scenarios"~\cite{Corgozinho2023}, which is also less "mundane"~\cite{Loescher2017}. It therefore reduces costs related to testing including change induced costs~\cite{Chen2022,Loescher2017}. Furthermore it allows for validating a software based on a much larger range of inputs~\cite{Loescher2017} [Hypothesis for Software Testing Research] and even more creative or sophisticated inputs~\cite{Arts2015}. Therefore PBT complements traditional testing techniques by unveiling yet unknown bugs within even well tested systems~\cite{Arts2015,Hughes2016,Arts2006} and in general, is useful for finding bugs within the implementation of a software and its specifications~\cite{Chen2022,Fink1997,Loescher2017,Paraskevopoulou2015,Claessen2000,Corgozinho2023}.

Although PBT offers quite some advantages, it does not come without any disadvantages. Due to the randomness of the input generator provided by tools the chances of finding more specific bugs reduces depending on the portion of erroneous inputs of the entire input range and therefore might fail to unveil errors~\cite{Loescher2017,Padhye2019,ElazarMittelman2023,Shi2023}. Not to mention that an enormous amount of tests processed implies reduced efficiency~\cite{ElazarMittelman2023,Shi2023}, as you try to approximate a formal proof using many randomly picked scenarios~\cite{Fink1997,ElazarMittelman2023,Paraskevopoulou2015}. Löscher (2017) gave quite a good fictional example using a "system of network nodes"~\cite{Loescher2017}. They tried to falsify the property that for any input scenarios (graphs created), the longest of the shortest paths "between the sink and other nodes [...] should not exceed 21 hops"~\cite{Loescher2017}. Even after "100000 tests"~\cite{Loescher2017} they were not able to falsify the property, which could be done "by hand"~\cite{Loescher2017}. A possible solution to this problem is implied by the usage of individually conceptualised data generators~\cite{Loescher2017,ElazarMittelman2023,Shi2023,Paraskevopoulou2015,Claessen2000} or targeted property-based testing~\cite{Loescher2017}. While constraining data generators by using domain knowledge in order to reduce the e.g. by using pre-conditions to cancel a test~\cite{Loescher2017,ElazarMittelman2023,Shi2023}, targeted property-based testing tries to guide the input generator "with search techniques towards values, that have a higher probability of falsifying a property"~\cite{Loescher2017}. Especially the first mentioned technique comes with its own challenges caused by the required development of your own generators, resulting in a reduced attractiveness of PBT~\cite{Loescher2017,ElazarMittelman2023,Shi2023}.

\subsubsection{Position in the Test Pyramid}
In order to position PBT at a level of the testing pyramid, we focus on the level of unit and integration (service) testing~\cite{Aniche2022,Radziwill2020}. To the best of our knowledge, PBT has not been applied to test the highest lvl of either system or UI tests~\cite{Radziwill2020,Aniche2022}, thus the highest lvl of the testing pyramid is ignored in discussing the position of PBT within the test pyramid. The relevant levels are now shortly summed up.

Beginning with unit testing, "developers perform unit testing to ensure that each component correctly implements its design and is ready to be integrated into a system of components" [integration testing]. In other words, "testing is performed in isolation from other components" [integration testing] and focuses on "example-based"~\cite{Corgozinho2023} testing of individual parts -- the "smallest parts"~\cite{Aniche2022} -- of a system ~\cite{Hartmann2000,Corgozinho2023}.

Within integration tests, every component of a system is integrated with each other if needed, including relevant external components~\cite{Aniche2022,Hartmann2000,Radziwill2020}. The goal is to ensure that interaction between the given components works correctly and are therefore properly integrated~\cite{Hartmann2000,Aniche2022}.

Although most of the mentioned use cases of chapter 2 (how does PBT work) relate to individual components in the means of functions such as adding numbers, inserting nodes in binary search trees and the sorted list [müssen hier die indirekten Zitate rein?], PBT also offers possible application surrounding the physical and external components of a software~\cite{Chen2022}. Coming back to the authentication-functionality provided by a system, you can not only test the functionality of authentication but also the integration with said authentication services~\cite{Fink1997}. Furthermore PBT has already been applied in many different cases of testing RESTful APIs in the context of OpenAPIs~\cite{Karlsson2019}, telecom systems~\cite{Arts2006}, synchronisation services~\cite{Hughes2016} or AUTOSAR software~\cite{Arts2015}. In the case of QuickREST you can use the response codes in order to differentiate between invalid and URL-requests, which in the end lead to revealing a yet unknown "underspecification"~\cite{Karlsson2019} of a used OpenAPIS documentation~\cite{Karlsson2019}. Whereas in the case of AUTOSAR the testing of correct request processing unveiled a yet unknown bug in terms of task prioritisation~\cite{Arts2015}. Last but not least, entire crashes of addressed components can be perceived as well~\cite{Arts2006}.

Therefore the method of PBT can be located within the level of unit and  integration testing. It can be very well applied to testing individual components of a system, but is not limited to it, because testing the integration of components is also possible and is used for both in practice.

\subsubsection{Tools and Programming Languages}
As already mentioned, PBT enjoys wide ranging support in many different programming languages~\cite{Chen2022,Shi2023}. It is supported by Java (Quicktheories\footnote{https://github.com/quicktheories/QuickTheories, accessed: 21.01.2024}), coq (QuickChick\footnote{https://github.com/QuickChick/QuickChick, accessed: 21.01.2024}), Scala (ScalaCheck\footnote{https://scalacheck.org/, accessed: 21.01.2024}), Erlang (QuickCheck\footnote{http://www.quviq.com/products/erlang-quickcheck/, accessed: 21.01.2024} and PropEr\footnote{https://proper-testing.github.io/, accessed: 21.01.2024}), Haskell (QuickCheck\footnote{https://hackage.haskell.org/package/QuickCheck, accessed: 21.01.2024}), OCaml (QCheck\footnote{https://github.com/c-cube/qcheck/, accessed: 21.01.2024} and Crowbar\footnote{https://github.com/stedolan/crowbar, accessed: 21.01.2024}) to name a few examples~\cite{MacIver2016,Padhye2019,Paraskevopoulou2015,Arts2008,Papadakis2011,Claessen2000}. Obviously many of these tools were inspired by QuickCheck, being the tool popularising PBT. However in this work we focus on Hypothesis\footnote{https://hypothesis.works/, accessed: 21.01.2024}, a PBT implementing framework for Python. It is a framework receiving much attention recently~\cite{Corgozinho2023,MacIver2019}, which is compatible with pytest, unittest\footnote{https://docs.python.org/3/library/unittest.html, accessed: 21.01.2024} and "probably many others", while being open source "under the Mozilla Public License 2.0".\footnote{https://hypothesis.works/products/, accessed: 21.01.2024}

% [Pages 5-7]
\subsection{Introduction to Hypothesis for Python}
\subsubsection{Overview of Hypothesis}
Hypothesis is a Python library for Property-Based Testing. The focus is on having an easy way to test code with a wide variety of inputs. The main difference when writing test cases in Hypothesis, versus traditional unit tests, is that instead of
giving concrete input, performing the function, and asserting something about the result, the focus is on trying to make the assertions hold for all data matching a certain specification.

The advantages are that Hypothesis helps to discover edge cases and hidden bugs which were not thought of during typical testing. It also helps to make the tests more robust, seeing as a wide variety of inputs, some even random, are used.
Furthermore, it can help save time, by encompassing many traditional unit test cases into one PBT test case. And lastly, Hypothesis also integrates into other testing frameworks such as Pytest and nose.

In terms of disadvantages, Hypothesis might lead to longer execution times of test suites. This is due to a Hypothesis test case generating many more subordinate test cases and running them. Although Hypothesis tries to reproduce the input that caused the test case to fail, this might not always work and it can therefore sometimes be challenging to understand why a test case failed.
Lastly, one typically has less control when using PBT in comparison to a typical unit test \cite{HypothesisDocs}.
% \begin{itemize}
%   \item Provide a brief introduction to Hypothesis for Python.
%   \item Mention its key features and advantages.
% \end{itemize}

\subsubsection{How to Use Hypothesis}
To use Hypothesis, start by installing the Python package. This can be done by using the command \texttt{pip install hypothesis}. In this project, the following setup was used:

\begin{itemize}
  \item Operating System: Ubuntu 22.04.3 LTS
  \item Python Version: 3.10.12
  \item pip Version: 22.0.2
\end{itemize}


However, officially, Hypothesis tries to support the latest version of Python \cite{HypothesisDocs}.


Consider the following code snippet:
\begin{lstlisting}[language=Python]
  from hypothesis import given
  import hypothesis.strategies as st
  
  @given(st.integers())
  def test_builtin_abs(x: int) -> None:
      assert abs(x) >= 0
      assert abs(x) == (x if x >= 0 else -x)
\end{lstlisting}

The main way in which the Hypothesis test is annotated, is using the given decorator. The given decorator takes a search strategy object
and uses this to populate the parameters of the function. A search strategy object refers to how the input for the function should be generated.
In this case, integers are generated to be used as input to the x parameter of the function.

This function then asserts that certain properties hold for each value of x generated. In this case, it tests that the built-in absolute value function of Hypothesis works
as it is intended to work.

Another key aspect of Hypothesis is being able to create new, unique search strategies. The following code block
is an example of this:

\begin{lstlisting}[language=Python]
  from hypothesis.strategies import composite

  PI = 3.14159
  
  @composite
  def custom_input_generator(draw) -> tuple[float, str]:
      decimal = draw(st.floats(max_value=PI))
      text = draw(
        st.text(alphabet=st.characters
        (whitelist_categories=['Lu']), 
      min_size=2, max_size=5))
      return decimal, text
  
  @given(custom_input_generator())
  def test_custom_input_generator
          (generated_input: tuple[float, str]) -> None:
      decimal, text = generated_input
      assert decimal <= PI
      assert len(text) >= 2 and len(text) <= 5
      assert text.isupper()
  
\end{lstlisting}

The composite decorator is used to specify a function that generates a custom search strategy.
The function works by combining existing search strategies. So in the above, the float search strategy
is used to generate floats up until a max value of PI. Then, the text search strategy is used to generate text
that consists of only upper-case letters and is between 2 and 5 characters long.

The test function simply tests that the search strategy generation function generates output of the correct form.

% \begin{itemize}
%   \item Write a mini how-to guide on using Hypothesis, including integration with pytest.
%   \item Include code snippets for better understanding.
% \end{itemize}

% [Pages 7-9]
\subsection{Main Concepts and Features of Hypothesis}
\subsubsection{Strategies and Data Generation}
One of the main concepts of Hypothesis is the idea of search strategies. Search strategies refer to how hypothesis will
try to generate input for the test function. Or rather, what it will use to "search" for bugs.

Hypothesis has plenty of different search strategies that can be used. For instance, it can generate text, floats, integers, boolean values, a value of a given set of values and values that match a given regex.
Furthermore, these search strategies can be refined by input parameters. This can for instance, limit the size of the floats generated or allow only dates between date ranges.

The format of using a search strategy in Hypothesis is always the same. Typically, an object representing a search strategy is instantiated using the syntax \texttt{st.(type)}, where \texttt{type} is the specific search strategy to use (e.g., \texttt{text}, \texttt{float}, \texttt{date}). The instantiation can be further refined by specifying parameters. The parameters can for instance, limit the dates to a certain range or only generate floats up to a certain size.

% \begin{itemize}
%   \item Explain the concept of strategies in Hypothesis for generating test data.
% \end{itemize}

% \subsubsection{Property-Based Testing with pytest}

% \begin{itemize}
%   \item Detail how Hypothesis integrates with pytest.
%   \item Provide examples of test functions using Hypothesis.
% \end{itemize}

\subsubsection{Data Analysis Applications and Benefits}
Hypothesis can be used to test data analysis scripts. In particular, Hypothesis can be really helpful to ensure that
the data preparation and data cleaning steps are properly tested. This is due to Hypothesis being able to simulate
a wide range of inputs, which can then be used to ensure that the functions are robust.

% \begin{itemize}
%   \item Discuss how data analysis applications can benefit from Hypothesis.
%   \item Reference specific features, such as the support for NumPy.
% \end{itemize}

% [Pages 9-13]
\subsection{Application of Hypothesis in Data Analysis}
\subsubsection{Code Examples}
% \begin{itemize}
%   \item Provide practical code examples demonstrating the use of Hypothesis in data analysis scripts.
%   \item Showcase scenarios where property-based testing adds value.
% \end{itemize}
The Data Analysis script that Hypothesis will be applied to, is a script that was provided by the DLR \cite{Stoffers2021Astronaut}.
The first function that will be examined is the Calculate Age function:

\begin{lstlisting}[language=Python]
  def calculate_age(born):
    today = date.today()
    return today.year - born.year - 
      ((today.month, today.day) < (born.month, born.day))
\end{lstlisting}

This is an auxiliary function, that is takes a date object and calculates the current age, by considering the time
that has passed since the given date. This function can be tested as follows:

\begin{lstlisting}[language=Python]
@given(st.dates(min_value=date(1920, 1, 1), 
                    max_value=date.today()))
def test_calculate_age(born: date) -> None:
  age = calculate_age(born)
  assert age >= 0 and 
  age <= (born.today().year - born.year)
\end{lstlisting}

The test generates random dates between the 1st of January 1920 and the current date and then
it confirms that the calculate age function works as intended.

The next function that will be tested is arguably the most important function in the script, the function that is used to prepare the data sets.
This what the function looks like:

\begin{lstlisting}[language=Python]
  df = rename_columns(df)
  df = df.set_index("astronaut_id")

  # Set pandas dtypes for columns with date or time
  df = df.dropna(subset=["time_in_space"])
  df["time_in_space"] = df["time_in_space"].astype(int)
  df["time_in_space"] = pd.to_timedelta
  (df["time_in_space"], unit="m")
  df["birthdate"] = pd.to_datetime(df["birthdate"])
  df["date_of_death"] = pd.to_datetime
  (df["date_of_death"])
  df.sort_values("birthdate", inplace=True)

  # Calculate extra columns from the original data
  df["time_in_space_D"] = df["time_in_space"] 
  / pd.Timedelta(days=1) # df["time_in_space_D"] = 
  df["time_in_space"].astype("timedelta64[D]")
  df["alive"] = df["date_of_death"].apply(is_alive)
  df["age"] = df["birthdate"].apply(calculate_age)
  df["died_with_age"] = df.apply(died_with_age, axis=1)
\end{lstlisting}

Please note that the line
\begin{equation}
  \texttt{df["time\_in\_space\_D"] = df["time\_in\_space"].astype("timedelta64[D]")}
\end{equation}
from the original script has been altered to
\begin{equation}
  \texttt{df["time\_in\_space"] / pd.Timedelta(days=1)}
\end{equation}
in the revised version.
Hypothesis is well-suited to test even a complicated function like the above. The focus
will be on generating appropriate inputs to the above.

\begin{lstlisting}[language=Python]
  @st.composite
  def astronaut_data(draw) -> dict:
      astronaut = draw(st.from_regex(
        r"http://www\.wikidata\.org/entity/Q\d+",
         fullmatch=True))
      astronautLabel = draw(
        st.from_regex(r"[A-Z][a-z]+ [A-Z][a-z]+"
        , fullmatch=True))
      birthdate = draw(
        st.dates(min_value=date(1920, 1, 1), 
                 max_value=date(2030, 12, 31)))
      birthplaceLabel = draw(
        st.from_regex(r"[A-Z][a-z]+", fullmatch=True))
      sex_or_genderLabel = draw(
        st.sampled_from(["male", "female"]))
      time_in_space = draw(
        st.integers(min_value=1, max_value=900)) 
      date_of_death = draw(st.one_of(
          st.none(), 
          st.dates(birthdate + timedelta(days=1), 
                   max_value=date(2030, 12, 31))
          )
      )
  
      birthdate_str = birthdate.strftime(
        "%Y-%m-%dT00:00:00Z")
      date_of_death_str = date_of_death.strftime(
        "%Y-%m-%dT00:00:00Z") if date_of_death else None
  
      return {
          "astronaut": astronaut,
          "astronautLabel": astronautLabel,
          "birthdate": birthdate_str,
          "birthplaceLabel": birthplaceLabel,
          "sex_or_genderLabel": sex_or_genderLabel,
          "time_in_space": time_in_space,
          "date_of_death": date_of_death_str
      }
\end{lstlisting}

This function generates a wide variety of inputs. Specifically, it generates an astronaut link and label using regex search strategies.
It then generates a birthdate that falls within a certain range, using the st.dates strategy as well as by specifiying a min and max value.
The sex or gender label is then generated using st.sampled from which then allows Hypothesis to generate input using the values male and "female".
The time in space is then generated using st.integers() and constraint to a given range.
Then the date of death is used by combining two strategies, st.none and st.dates, of which one will be selected.
The birthdate and if present death date, are then transformed into the corresponding format that the data preparation script expects.

This shows how Hypothesis can be used to generate complex inputs which can then be used to test data analysis scripts.
This will result in the following output.

\subsubsection{Illustrative Cases}
% \begin{itemize}
%   \item Present specific cases where Hypothesis helped discover issues in data analysis scripts.
% \end{itemize}
Hypothesis can also be very helpful in finding potential bugs and making data analysis scripts more robust. In particular, by removing the max value of the
time in space strategy, Hypothesis will trigger the data analysis script to crash. The following output is returned by Hypothesis:

\begin{verbatim}
  OverflowError: Python int too large to convert to C long
Falsifying example: test_prepare_data_set(
    data=[{'astronaut': 'http://www.wikidata.org/entity/Q0',
      'astronautLabel': 'Aa Aa',
      'birthdate': '2000-01-01T00:00:00Z',
      'birthplaceLabel': 'Aa',
      'sex_or_genderLabel': 'male',
      'time_in_space': 18446744073709551616,
      'date_of_death': '2000-01-02T00:00:00Z'}],
)

\end{verbatim}

The specific can be traced back to :

\begin{lstlisting}[language=Python]
  ....
  df = rename_columns(df)
  df = df.set_index("astronaut_id")

  # Set pandas dtypes for columns with date or time
  df = df.dropna(subset=["time_in_space"])
  df["time_in_space"] = df["time_in_space"]
  .astype(int) # This caused line caused the error
  df["time_in_space"] = pd.to_timedelta
  (df["time_in_space"], unit="m")
  df["birthdate"] = pd.to_datetime(df["birthdate"])
  df["date_of_death"] = pd.to_datetime
  (df["date_of_death"])
  df.sort_values("birthdate", inplace=True)

....
\end{lstlisting}

And it is due to a Python integer being to large to be converted to a C long. The data preparation script can
therefore be improved, by checking beforehand that the integers are within a specified range.

As for another example, by removing the max date range, the following error will be produced:

\begin{verbatim}
  pandas._libs.tslibs.np_datetime.OutOfBoundsDatetime: 
  Out of bounds nanosecond timestamp: 2263-01-01T00:00:00Z, 
  at position 0
  Falsifying example: test_prepare_data_set(
      data=[{'astronaut': 'http://www.wikidata.org/entity/Q0',
        'astronautLabel': 'Aa Aa',
        'birthdate': '2000-01-01T00:00:00Z',
        'birthplaceLabel': 'Aa',
        'sex_or_genderLabel': 'male',
        'time_in_space': 1,
        'date_of_death': '2263-01-01T00:00:00Z'}],
  ) 
\end{verbatim}

The datetime64[ns] data type in pandas/Numpy is limited to the range of dates from 1677-09-21 to 2262-04-11 because it stores dates as 64-bit integers representing nanoseconds since the Unix epoch (January 1, 1970). Therefore, if numpy tries to convert the date of death, '2263-01-01T00:00:00Z', the script will crash.
This shows yet another example of how the data script can be made more robust. Specifically, be checking that the dates are within date ranges that
pandas/Numpy can process.


\section{Discussion}

% [Page 13-14]
\section{Conclusion}
\begin{itemize}
  \item Summarize the key points discussed in the paper.
  \item Emphasize the importance of property-based testing, particularly with tools like Hypothesis, in enhancing the reliability of data analysis scripts.
\end{itemize}
%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{references}
\end{document}
