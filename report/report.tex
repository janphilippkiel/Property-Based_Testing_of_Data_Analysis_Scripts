% SPDX-FileCopyrightText: 2023 Jean-Sebastian de Wet, Jan-Philipp Kiel, Pascal Mager
% SPDX-License-Identifier: CC-BY-4.0
%
% LLNCS macro package for Springer Computer Science proceedings Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
%
\usepackage{listings}
\AtBeginDocument{\counterwithin{lstlisting}{section}%
  \renewcommand\thelstlisting{\arabic{lstlisting}}}
\renewcommand{\lstlistingname}{Code Example}
\lstset{
  aboveskip=1em,
  breaklines=true,
  captionpos=b,
  escapeinside=||,
  frame=single,
  numbers=left,
  numbersep=1em,
  numberstyle=\tiny,
  showlines=true,
}
\let\origthelstnumber\thelstnumber
\makeatletter
\newcommand*\Suppressnumber{%
  \lst@AddToHook{OnNewLine}{%
    \let\thelstnumber\relax%
     \advance\c@lstnumber-\@ne\relax%
    }%
}
\newcommand*\Reactivatenumber[1]{%
  \setcounter{lstnumber}{\numexpr#1-1\relax}
  \lst@AddToHook{OnNewLine}{%
   \let\thelstnumber\origthelstnumber%
   \refstepcounter{lstnumber}%
  }%
}
\makeatother
% Used to display Python code.
%
\begin{document}
%
\title{Property-Based Testing of Data Analysis Scripts}
\subtitle{A Focus on Hypothesis for Python}
%\subtitle{Enhancing Reliability in DLR Research}  % Alternative subtitle
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Jean-Sebastian de Wet \and
  Jan-Philipp Kiel \and
  Pascal Mager}
%
\authorrunning{Group Criterion}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{University of Cologne, Cologne, Germany\\Group Criterion}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
  This report explores property-based testing (PBT) as a meth\-od to enhance the reliability of data analysis scripts, particularly for research at the German Aerospace Center (DLR) using Python. It outlines challenges with traditional testing in scenarios with diverse data values, emphasising the need for innovative testing approaches. The literature study thoroughly covers PBT, including its history, key concepts, advantages and disadvantages, as well as its classification within the test pyramid. It then focuses on a prototype using Hypothesis for Python, detailing its functionalities, integration with pytest, and unique features. Real-world application is demonstrated through code examples showcasing how Hypothesis strengthens the reliability of data analysis scripts. The report concludes that while PBT shows promise in enhancing software testing, further studies are needed to assess its full potential in research-intensive settings like DLR.

  \keywords{Property-Based Testing \and Data Analysis Scripts \and Hypothesis \and Python \and pytest \and Reliability \and Test Pyramid \and Code Examples \and DLR Research.}
\end{abstract}
%
%
%
\section{Introduction}
In the evolving landscape of data analysis, the complexity and volume of datasets have grown exponentially~\cite{Taylor2023}, presenting unique challenges across various fields. This surge in data complexity necessitates robust testing methodologies to ensure the accuracy and reliability of data analysis tools and scripts. Traditional testing approaches, primarily based on specific input-output cases, often fall short in addressing the dynamic and unpredictable nature of modern datasets. These limitations are particularly evident in specialised fields like aerospace research, where the data's scope and diversity are exceptionally vast.

The structure of this report begins with a brief description of the applied methodology, followed by the presentation of results. These results are then discussed in detail, placed in the context of the existing literature and used to derive implications for future works.

\section{Background}
Within the German Aerospace Center (DLR), the reliability of data analysis scripts is a cornerstone of successful research outcomes. DLR researchers frequently use Python~\cite{Kurnatowski2020}, along with its powerful libraries like pandas\footnote{https://pandas.pydata.org/, accessed: 2024-01-21} and Matplotlib\footnote{https://matplotlib.org/, accessed: 2024-01-21}, for complex data manipulations and visualisations. This introduces significant testing challenges. DLR, being at the forefront of aerospace research and development, deals with an enormous range of data variables, from satellite imagery to flight dynamics.\footnote{https://www.dlr.de/en/dlr/about-us, accessed: 2024-01-21} This variety and complexity of data make testing particularly challenging. This report explores the adoption of property-based testing, presenting it as an innovative and essential strategy to overcome these testing challenges, especially in scenarios involving large possible value ranges of data.

\section{Method}
Our methodology in tackling these challenges involved two key stages.

\subsection{Literature Study}
The first stage of our methodology involved a thorough literature review on property-based testing. Our objective was to gain an in-depth understanding of its theoretical foundations, including its history, key principles, and diverse applications. The process entailed systematic searches in academic databases and online repositories, using targeted keywords such as `property-based testing' and `automatic test case generation'. We focused on selecting literature from various domains, particularly papers highlighting PBT's ability to create diverse test cases. This approach provided a solid knowledge base and included insights on handling complex datasets in DLR. These findings are complemented in the next stage of our study.

\subsection{Prototyping}
The second stage centered on practical application, where we developed a comprehensive guide for using Hypothesis, a property-based testing framework for Python. This guide covered critical aspects including installation, configuration, and integration with Python's pytest\footnote{https://pytest.org/, accessed: 2024-01-21} framework. It also provided practical examples demonstrating the process of defining properties and generating test cases using Hypothesis. To validate our methodology, we conducted a case study that involved applying property-based testing to a data analysis script related to astronauts. This case study served as a real-world application, illustrating the practicality of property-based testing in complex data scenarios.

\section{Results}
First, we uncovered key theoretical insights on property-based testing through our literature study. Then, we demonstrated the practical effectiveness of this approach in our prototype using Hypothesis.

\subsection{Literature Study of Property-Based Testing}
\subsubsection{History}
The origins of property-based testing (PBT) can be traced back more than 20 years ago, even before 2000. While it had already been a subject within information technology research, as seen in Guo (1999) and Fink (1997), it gained significantly more attention with the development of QuickCheck\footnote{https://www.cse.chalmers.se/\%7Erjmh/QuickCheck/, accessed: 2024-01-21}~\cite{Shi2023,Guo1999,Fink1997,MacIver2019,Honarvar2020}. Initially focusing on research questions related to topics such as automation of test input generation and automated techniques in general~\cite{Fink1997} and directing the automated input generator towards values with a higher probability of failure~\cite{Loescher2017}, recent papers have focused on the implementation of different frameworks or platforms and techniques for PBT application~\cite{Padhye2019,Honarvar2020,Shi2023,Corgozinho2023}. To top it off, PBT nowadays enjoys wide-ranging support in different programming languages including automation capabilities~\cite{Chen2022,Padhye2019,Honarvar2020,ElazarMittelman2023,Shi2023}, as well as the application within many different python projects using Hypothesis as a framework for PBT~\cite{Corgozinho2023}.

\subsubsection{Key Concepts}
PBT is a method supporting the formal verification of a software~\cite{Chen2022,Fink1997,Honarvar2020,Paraskevopoulou2015}, focusing on validating high-level or general properties of the software~\cite{Fink1997,Honarvar2020,Corgozinho2023}. Test cases within this method are typically formulated using logical descriptions of the expected behavior of the software~\cite{Chen2022,Fink1997,Honarvar2020,Loescher2017,Corgozinho2023}, including pre- or post-conditions of the system~\cite{Honarvar2020}. To formally validate the system's behavior, a single test case is executed multiple times with randomly generated input to search for counterexamples that violate specific properties or cause a software crash, thereby invalidating said property~\cite{Chen2022,Loescher2017,Padhye2019,ElazarMittelman2023,Paraskevopoulou2015,Corgozinho2023}. Data generators are used for random input generation, which can be adjusted based on a certain domain's needs~\cite{Chen2022,Loescher2017,Padhye2019,ElazarMittelman2023}. Through automated execution of tests with random input, PBT tries to approximate the validity of a property by subjecting it to numerous instantiations within a given input range; otherwise, the property is falsified~\cite{Fink1997,ElazarMittelman2023,Corgozinho2023,Paraskevopoulou2015}. To further elaborate on the properties, which represent the desired behaviour in terms of input and output of given tested functions through specifications~\cite{Chen2022,Fink1997,Loescher2017}, some examples can be given. To start with a simple one, think of a function that adds two numbers (A, B) and returns the sum of both numbers (A + B). You can define a test which asserts that for any given input for either A and B, the function will return the addition of both numbers. Another simple example would be a sort-function. Assume a sort-function sorts a list, the invariant would be, that sorting an already sorted or the same list twice, will result in a sorted and the same list~\cite{Corgozinho2023}. A common example which is frequently used for showcasing PBT are binary search trees~\cite{Corgozinho2023,Shi2023}, where one property to test might be that after ``insert[ing] a key into a valid BST, it should maintain its validity''~\cite{Shi2023}. In the context of PBT you would then use logic expressions for your tests and use a random input generator to check whether the given property is violated in any case within a certain input range. It is also possible to apply it to software security concerns such as authentication~\cite{Fink1997} or for verifying the ``correctness of hardware [and] external software involved''~\cite{Chen2022}. In summary, PBT allows for the approximation of a system's invariants formal verification~\cite{Fink1997,ElazarMittelman2023,Corgozinho2023}.

\subsubsection{Advantages and Disadvantages}
As previously mentioned, PBT supports the formal validation of software by testing specified properties using randomly generated input for each test. However, describing all expected behavior of a system in a logical style is often less feasible~\cite{Chen2022,Koopman2012}. In contrast, by applying PBT the required endeavour for formal validation can be lowered~\cite{Hritcu2016,Chen2022,Paraskevopoulou2015}. Moreover, specifications used for PBT might also improve cooperation between software developers and testers in larger projects, as the language used for defining tests is easier to grasp compared to abstract proofs~\cite{Chen2022,Loescher2017}. Besides this, PBT can also be applied to test in several contexts~\cite{Karlsson2019}, such as interfaces~\cite{Karlsson2019,Francisco2013,LamelaSeijas2013}, e.g. by testing invariants regarding the responses of requested URLs of REST-APIs~\cite{Karlsson2019}. Other potential domains of application include telecom systems~\cite{Arts2006}, file synchronisation services~\cite{Hughes2016} and databases~\cite{Arts2015}. Despite its wide applicability and advantages in formal validation, PBT reduces engineering effort in terms of defining individual test cases and input parameters~\cite{Chen2022,Loescher2017,Corgozinho2023} and may also incentivise developers to design code that can be easily expressed by properties~\cite{Chen2022}. More specifically, when compared to manually written tests like in unit-testing, PBT allows the software engineer to put more emphasis on ensuring and restoring correctness of the software and less on ``defining test case inputs, examples, and scenarios''~\cite{Corgozinho2023}, turning it into a much less ``mundane''~\cite{Loescher2017} effort. It therefore reduces costs related to testing, including costs induced by changes made to the software~\cite{Chen2022,Loescher2017}. Furthermore it allows for validating a software based on a much larger range of inputs~\cite{Loescher2017} [Hypothesis for Software Testing Research] and even more creative or sophisticated inputs~\cite{Arts2015}. Therefore PBT complements traditional testing techniques by unveiling yet unknown bugs within even well tested systems~\cite{Arts2015,Hughes2016,Arts2006} and in general, is useful for finding bugs within the implementation of a software and its specifications~\cite{Chen2022,Fink1997,Loescher2017,Paraskevopoulou2015,Claessen2000,Corgozinho2023}.

Although PBT offers quite some advantages, it does not come without any disadvantages. Due to the randomness of the input generator provided by tools, the chances of finding more specific bugs is reduced, depending on the portion of erroneous inputs of the entire input range, thus potentially failing to unveil errors~\cite{Loescher2017,Padhye2019,ElazarMittelman2023,Shi2023}. It should be noted that processing a large number of tests can lead to reduced efficiency~\cite{ElazarMittelman2023,Shi2023}, as it involves attempting to approximate a formal proof using numerous randomly selected scenarios~\cite{Fink1997,ElazarMittelman2023,Paraskevopoulou2015}. LÃ¶scher (2017) gave quite a good fictional example using a ``system of network nodes''~\cite{Loescher2017}. They tried to falsify the property that for any input scenarios (graphs created), the longest of the shortest paths ``between the sink and other nodes [...] should not exceed 21 hops''~\cite{Loescher2017}. Even after ``100000 tests''~\cite{Loescher2017} they were not able to falsify the property, which could be done ``by hand''~\cite{Loescher2017}. A possible solution to this problem is implied by the usage of individually conceptualised data generators~\cite{Loescher2017,ElazarMittelman2023,Shi2023,Paraskevopoulou2015,Claessen2000} or targeted property-based testing~\cite{Loescher2017}. While constraining data generators in order to cancel a test by using pre-conditions as a form of domain knowledge represents an option to reduce computation effort~\cite{Loescher2017,ElazarMittelman2023,Shi2023}, targeted property-based testing achieves this by guiding the input generator~\cite{Loescher2017}. The idea is to increase the chance of generating inputs causing the violation of a property, by applying ``search techniques''~\cite{Loescher2017}. However, the first technique of implementing specified data generators comes with its own challenges regarding the development~\cite{Loescher2017,ElazarMittelman2023,Shi2023}, resulting in a reduced attractiveness of PBT.

\subsubsection{Classification within the Test Pyramid}
In order to classify PBT within the test pyramid, we focus on the level of unit and integration (service) testing~\cite{Aniche2022,Radziwill2020}. Current literature indicates that PBT has not yet been applied for testing at the highest level of either system or UI tests~\cite{Radziwill2020,Aniche2022}, making them irrelevant for our analysis.

\begin{figure}
  \includegraphics[width=.618\textwidth]{includes/test_pyramid.eps}
  \caption{Test Pyramid. Based on Vocke, H.: The Practical Test Pyramid (2018). https://martinfowler.com/articles/practical-test-pyramid.html}
  \label{fig:test_pyramid}
\end{figure}

Beginning with unit testing, ``developers perform unit testing to ensure that each component correctly implements its design and is ready to be integrated into a system of components''~\cite{Hartmann2000}. In other words, each function or module of a system is tested in an isolated manner. Furthermore it emphasised on ``example-based''~\cite{Corgozinho2023} testing individual features or functions ---also referred to as the ``smallest parts''~\cite{Aniche2022}---of a system~\cite{Hartmann2000,Corgozinho2023}.

Within integration tests, as the name suggests, every component of a system is integrated with each other, including relevant external components~\cite{Aniche2022,Hartmann2000,Radziwill2020}. The goal is to ensure that interaction between the given components works correctly and therefore, the combined set of functionality~\cite{Hartmann2000,Aniche2022}.

Although the majority of the use cases explaining how PBT works focus on individual components, such as functions for adding numbers, inserting nodes in binary search trees,
and managing sorted lists, it also offers potential applications concerning the physical and external aspects of software~\cite{Chen2022}. Referring back to the example regarding authentication, one can test not only the authentication functionality of a system but also the integration of its respective authentication services~\cite{Fink1997}. Additionally, PBT has been utilised in various scenarios such as testing RESTful APIs within the context of OpenAPIs~\cite{Karlsson2019}, telecom systems~\cite{Arts2006}, synchronisation services~\cite{Hughes2016} or AUTOSAR software~\cite{Arts2015}. For instance, in the case of QuickREST, response codes can be used to differentiate between invalid and URL requests, ultimately revealing previously unknown `underspecification' in the utilised OpenAPI documentation~\cite{Karlsson2019}. Similarly, in the context of AUTOSAR, testing correct request processing unveiled a previously unknown bug related to task prioritisation~\cite{Arts2015}. Lastly, complete crashes of addressed components can also be observed~\cite{Arts2006}.

Consequently, the PBT method can be applied to both unit and integration testing. In practical terms, PBT is commonly employed for unit testing and is well suited for this purpose, as its highlighted advantages demonstrate complementary effects compared to simple example-based unit testing. However, its applicability is not restricted solely to unit testing. Existing literature has shown that it also extends to testing the integration of various components.

\subsubsection{Tools and Programming Languages}
As previously mentioned, PBT is widely supported across various programming languages~\cite{Chen2022,Shi2023}. It is supported by Java (Quicktheories\footnote{https://github.com/quicktheories/QuickTheories, accessed: 2024-01-21}), coq (QuickChick\footnote{https://github.com/QuickChick/QuickChick, accessed: 2024-01-21}), Scala (ScalaCheck\footnote{https://scalacheck.org/, accessed: 2024-01-21}), Erlang (QuickCheck\footnote{http://www.quviq.com/products/erlang-quickcheck/, accessed: 2024-01-21} and PropEr\footnote{https://proper-testing.github.io/, accessed: 2024-01-21}), Haskell (QuickCheck\footnote{https://hackage.haskell.org/package/QuickCheck, accessed: 2024-01-21}), OCaml (QCheck\footnote{https://github.com/c-cube/qcheck/, accessed: 2024-01-21} and Crowbar\footnote{https://github.com/stedolan/crowbar, accessed: 2024-01-21}) to name a few examples~\cite{MacIver2016,Padhye2019,Paraskevopoulou2015,Arts2008,Papadakis2011,Claessen2000}. Obviously many of these were inspired by QuickCheck, being the tool popularising PBT. However, this study focuses on Hypothesis\footnote{https://hypothesis.works/, accessed: 2024-01-21}, a Python-based PBT implementation framework. This framework has garnered significant attention recently~\cite{Corgozinho2023,MacIver2019}, is compatible with pytest, unittest\footnote{https://docs.python.org/3/library/unittest.html, accessed: 2024-01-21} and ``probably many others'', as well as being open source ``under the Mozilla Public License 2.0''\footnote{https://hypothesis.works/products/, accessed: 2024-01-21}.

\subsection{Prototype of Data Analysis Scripts using Hypothesis for Python}
\subsubsection{Overview}
Hypothesis is a Python library for Property-Based Testing. The aim is to have a simple manner in which software can be tested. In traditional unit testing the focus is on performing the function and asserting something about the result. With Hypothesis, the focus is on trying to make the assertions hold for all data matching a certain specification.

The advantages are that Hypothesis helps to discover edge cases and hidden bugs which one might not think of during typical testing. It also helps to make the tests more robust, seeing as a wide variety of inputs, some even random, are used.
Furthermore, it can help save time, by encompassing many traditional unit test cases into one PBT test case. And lastly, Hypothesis also integrates into other testing frameworks such as pytest and nose. In terms of disadvantages, Hypothesis might lead to longer execution times of test suites. This is due to a Hypothesis test case generating many more subordinate test cases and running them. Although Hypothesis tries to reproduce the input that caused the test case to fail, this might not always work and it can therefore sometimes be challenging to understand why a test case failed.
Lastly, one typically has less control when using PBT in comparison to a typical unit test.\footnote{https://hypothesis.readthedocs.io/en/latest/, accessed: 2024-01-10}. The majority of these advantages and disadvantages correspond with the general advantages and disadvantages of PBT that can be found within the academic literature.

\subsubsection{Strategies and Data Generation}
One of the main concepts of Hypothesis is the idea of search strategies. Search strategies refer to how hypothesis will try to generate input for the test function. Or rather, what it will use to `search' for bugs. Hypothesis has plenty of different search strategies that can be used. For instance, it can generate text, floats, integers, boolean values, a value of a given set of values and values that match a given regex. Furthermore, these search strategies can be refined by input parameters. This can, for instance, limit the size of the floats generated or allow only dates between date ranges.

The format of using a search strategy in Hypothesis is consistent. Typically, an object representing a search strategy is instantiated using the syntax \texttt{st.(type)}, where \texttt{type} is the specific search strategy to use (e.g., \texttt{text}, \texttt{float}, \texttt{date}). The instantiation can be further refined by specifying parameters. The parameters can for instance, limit the dates to a certain range or only generate floats up to a certain size. By default, Hypothesis will try to create 100 different tests

\subsubsection{Hypothesis Usage}
In order to use Hypothesis, one first needs to install the Hypothesis package. This can be done by using the command \texttt{`pip install hypothesis'}. The specific setup used is as follows:

\begin{itemize}
  \item Operating System: Ubuntu 22.04.3 LTS
  \item Python Version: 3.10.12
  \item pip Version: 22.0.2
  \item hypothesis Version: 6.92.2
\end{itemize}
However, Hypothesis officially tries to support the latest version of Python.\footnote{https://hypothesis.readthedocs.io/en/latest/, accessed: 2024-01-10}

\vspace{5mm}
\noindent Consider the following code extract:
\begin{lstlisting}[language=Python,caption={Basic Test from code/tutorial.ipynb}]
  from hypothesis import given
  import hypothesis.strategies as st

  @given(st.integers())
  def test_builtin_abs(x: int) -> None:
      assert abs(x) >= 0
      assert abs(x) == (x if x >= 0 else -x)

  test_builtin_abs()
\end{lstlisting}
The main way in which the Hypothesis test is annotated, is using the \texttt{given} decorator. The \texttt{given} decorator takes a search strategy object and uses this to populate the parameters of the function. In this case, integers are generated to be used as input to the $x$ parameter of the function.

This function then asserts that certain properties hold for each value of $x$ generated. In this example, it tests that the built-in absolute value function of Hypothesis works as intended.

\vspace{5mm}
\noindent Another key aspect of Hypothesis is being able to create new, unique search strategies. The following code block is an example of this:

\begin{lstlisting}[language=Python,caption={Complex Inputs from code/tutorial.ipynb}]
  from hypothesis.strategies import composite

  PI = 3.14159

  @composite
  def custom_input_generator(draw) -> tuple[float, str]:
    decimal = draw(st.floats(max_value=PI))
    text = draw(
      st.text(alphabet=st.characters
      (whitelist_categories=['Lu']), 
    min_size=2, max_size=5))
    return decimal, text

  @given(custom_input_generator())
  def test_custom_input_generator
        (generated_input: tuple[float, str]) -> None:
    decimal, text = generated_input
    assert decimal <= PI
    assert len(text) >= 2 and len(text) <= 5
    assert text.isupper()
\end{lstlisting}
The \texttt{composite} decorator is used to specify a function that generates a custom search strategy. The function works by combining existing search strategies. So in the above, the float search strategy is used to generate floats up until a max value of $\pi$. Then, the text search strategy is used to generate text that consists of only upper-case letters and is between 2 and 5 characters long. The test function simply tests that the search strategy generation function generates output of the correct form.

\vspace{5mm}
\noindent In order to improve test robustness, it can be useful to increase the number of tests generated. Furthermore, generating large amounts of random test cases helps to approximate formal verification. The number of test cases that are generated can be changed with the \texttt{settings} decorator and the \texttt{max\_samples} property:

\begin{lstlisting}[language=Python,caption={Configuring Number of Generated Test Cases from code/tutorial.ipynb}]
  from hypothesis import settings

  @given(st.integers())
  @settings(max_examples=100)
  def test_builtin_abs(x: int) -> None:|\Suppressnumber|
    ...|\Reactivatenumber{8}|

\end{lstlisting}

\noindent Globally this can be done as follows in the beginning of the Python test file:
\begin{lstlisting}[language=Python,caption={Configuring Number of Generated Test Cases from code/tutorial.ipynb}]
  settings.register_profile("default", max_examples=100)
  settings.load_profile("default")
\end{lstlisting}

\noindent Lastly, another useful feature of Hypothesis is the ability to specify a seed. When using a seed, the test cases generated by Hypothesis will always be identical. This eliminates the random element. Seeds can be helpful to debug code, because Hypothesis will try to determine and present the specific seed that caused the program to fail. It might also be useful to have, in addition to randomized tests, specific seeds in the test suite in order to improve testing reliability. A seed can be set using the \texttt{seed} decorator and a specific seed value.

\newpage
\noindent Here is an example:
\begin{lstlisting}[language=Python,caption={Specifying Seeds in Hypothesis from code/tutorial.ipynb}]
from hypothesis import given, seed
import hypothesis.strategies as st

@seed(30)
@given(st.integers())
def test_builtin_abs(x: int) -> None:
    assert abs(x) >= 0
    assert abs(x) == (x if x >= 0 else -x)

test_builtin_abs()
\end{lstlisting}

\subsubsection{Integration with pytest}
The integration with pytest is automatic. It is sufficient to run pytest in the traditional sense, as pytest will automatically recognize the Hypothesis tests. In the \texttt{tutorial.ipynb} notebook this is illustrated. It can be seen that pytest correctly identified the Hypothesis test. It is, however, important to note that pytest registers a hypothesis test as a single test. In actuality, multiple tests are run. By using the \texttt{--hypothesis-show-statistics} flag, one can get more detailed information specific to Hypothesis. For example:
\begin{lstlisting}[language=bash]
  pytest test.py --hypothesis-show-statistics
\end{lstlisting}

\noindent The output returned is:
\begin{verbatim}
======================= Hypothesis Statistics =====================
test.py::test_builtin_abs:

  - during generate phase (0.02 seconds):
    - Typical runtimes: < 1ms, of which < 1ms in data generation
    - 100 passing examples, 0 failing examples, 0 invalid examples

  - Stopped because settings.max_examples=100
\end{verbatim}


\subsubsection{Application for Data Analysis Scripts}
Hypothesis can be used to test data analysis scripts. In particular, Hypothesis can be helpful to ensure that the data preparation and data cleaning steps are properly tested. This is due to Hypothesis being able to simulate a wide range of inputs, which can then be used to ensure that the functions are robust. In the following, we will apply Hypothesis to an exemplary astronaut data analysis script~\cite{Stoffers2021} from the DLR.

% \vspace{5mm}
\newpage
\noindent The first function that will be examined is the \texttt{calculate\_age} function:

\begin{lstlisting}[language=Python,caption={Calculate Age from code/data\_analysis.ipynb}]
  |\Suppressnumber|
  ...|\Reactivatenumber{7}|
  def calculate_age(born):
    today = date.today()
    return today.year - born.year - ((today.month, today.day) < (born.month, born.day))
\end{lstlisting}
This is an auxiliary function in the data preparation script. It takes a date object and calculates the current age by considering the time that has passed since the given date. This function can be tested as follows:

\begin{lstlisting}[language=Python,,caption={Calculate Age from code/data\_analysis.ipynb}]
  @given(st.dates(min_value=date(1920, 1, 1), 
                  max_value=date.today()))
  def test_calculate_age(born: date) -> None:
    age = calculate_age(born)
    assert age >= 0 and age <= (born.today().year - born.year)
  
  test_calculate_age()
\end{lstlisting}
The test generates random dates between the 1st of January 1920 and the current date. It then confirms that the \texttt{calculate\_age} function works as intended.

\vspace{5mm}
\noindent The next function that will be tested is arguably the most important function in the script, the function that is used to prepare the data sets:

\begin{lstlisting}[language=Python,caption={Prepare Data Set from code/data\_analysis.ipynb}]
  def prepare_data_set(df):
    df = rename_columns(df)
    df = df.set_index("astronaut_id")

    # Set pandas dtypes for columns with date or time
    df = df.dropna(subset=["time_in_space"])
    df["time_in_space"] = df["time_in_space"].astype(int)
    df["time_in_space"] = pd.to_timedelta(df["time_in_space"], unit="m")
    df["birthdate"] = pd.to_datetime(df["birthdate"])
    df["date_of_death"] = pd.to_datetime(df["date_of_death"])
    df.sort_values("birthdate", inplace=True)

    # Calculate extra columns from the original data
    df["time_in_space_D"] = df["time_in_space"] / pd.Timedelta(days=1)
    df["alive"] = df["date_of_death"].apply(is_alive)
    df["age"] = df["birthdate"].apply(calculate_age)
    df["died_with_age"] = df.apply(died_with_age, axis=1)
    return df|\Suppressnumber|
  ...|\Reactivatenumber{46}|

\end{lstlisting}

\vspace{5mm}
\noindent Please note that the original line from the script:
\begin{lstlisting}[language=Python,caption={Prepare Data Set from code/data\_analysis.ipynb},firstnumber=14]
  df["time_in_space_D"] = df["time_in_space"].astype("timedelta64[D]")
\end{lstlisting}
has been altered to:
\begin{lstlisting}[language=Python,caption={Prepare Data Set from code/data\_analysis.ipynb},firstnumber=14]
  df["time_in_space_D"] = df["time_in_space"] / pd.Timedelta(days=1)
\end{lstlisting}
in the revised version.

\vspace{5mm}
\noindent Hypothesis is well suited to test even a complicated function like the above. The focus will be on generating appropriate inputs for the \texttt{prepare\_data\_set} function:

\begin{lstlisting}[language=Python,caption={Prepare Data Set from code/data\_analysis.ipynb}]
@st.composite
def astronaut_data(draw) -> dict:
    astronaut = draw(st.from_regex(r"http://www\.wikidata\.org/entity/Q\d+", fullmatch=True))
    astronautLabel = draw(st.from_regex(r"[A-Z][a-z]+ [A-Z][a-z]+", fullmatch=True))
    birthdate = draw(st.dates(min_value=date(1920, 1, 1), max_value=date(2030, 12, 31)))
    birthplaceLabel = draw(st.from_regex(r"[A-Z][a-z]+", fullmatch=True))
    sex_or_genderLabel = draw(st.sampled_from(["male", "female"]))
    time_in_space = draw(st.integers(min_value=1, max_value=900)) 
    date_of_death = draw(st.one_of(
        st.none(), 
        st.dates(birthdate + timedelta(days=1), 
                  max_value=date(2030, 12, 31))
        )
    )|\Suppressnumber|
    ...|\Reactivatenumber{40}|

\end{lstlisting}
This function generates a wide variety of inputs. Specifically, it generates an astronaut link and label using regex search strategies. It then generates a birthdate that falls within a certain range using the \texttt{st.dates} strategy and specifying the date range. The \texttt{sex\_or\_genderLabel} is then generated using \texttt{st.sampled} which then allows Hypothesis to generate input using the values \texttt{male} and \texttt{female}. The time in space is then generated using \texttt{st.integers} and confined to a given range. Then the date of death is used by combining two strategies, \texttt{st.none} and \texttt{st.dates}, of which one will be selected. The birthdate and, if present, death date are then transformed into the corresponding format that the data preparation script expects. This shows how Hypothesis can be used to generate complex inputs which can then be used to test data analysis scripts.

\subsubsection{Discovering Issues}
Hypothesis can also be very helpful in finding potential bugs and making data analysis scripts more robust. In particular, by removing the max value of the time in space strategy, Hypothesis will trigger the data analysis script to crash. The following output is returned:

\begin{verbatim}
OverflowError: Python int too large to convert to C long
Falsifying example: test_prepare_data_set(
    data=[{'astronaut': 'http://www.wikidata.org/entity/Q0',
      'astronautLabel': 'Aa Aa',
      'birthdate': '2000-01-01T00:00:00Z',
      'birthplaceLabel': 'Aa',
      'sex_or_genderLabel': 'male',
      'time_in_space': 18446744073709551616,
      'date_of_death': '2000-01-02T00:00:00Z'}],
  )
\end{verbatim}

\noindent The specific can be traced back to:

\begin{lstlisting}[language=Python,caption={Prepare Data Set from code/data\_analysis.ipynb}]
  |\Suppressnumber|
  ...|\Reactivatenumber{5}|
  # Set pandas dtypes for columns with date or time
  df = df.dropna(subset=["time_in_space"])
  df["time_in_space"] = df["time_in_space"].astype(int)
  df["time_in_space"] = pd.to_timedelta(df["time_in_space"], unit="m")
  df["birthdate"] = pd.to_datetime(df["birthdate"])
  df["date_of_death"] = pd.to_datetime(df["date_of_death"])
  df.sort_values("birthdate", inplace=True)|\Suppressnumber|
  ...|\Reactivatenumber{46}|

\end{lstlisting}
The error is due to an `int' value being too large to be converted to a `C long'. The data preparation script can therefore be improved by ensuring that processed integers are within an error-free range.

\vspace{5mm}
\noindent As for another example, by removing the max date range the following error will be produced:

\begin{verbatim}
pandas._libs.tslibs.np_datetime.OutOfBoundsDatetime: 
Out of bounds nanosecond timestamp: 2263-01-01T00:00:00Z, 
at position 0
Falsifying example: test_prepare_data_set(
    data=[{'astronaut': 'http://www.wikidata.org/entity/Q0',
      'astronautLabel': 'Aa Aa',
      'birthdate': '2000-01-01T00:00:00Z',
      'birthplaceLabel': 'Aa',
      'sex_or_genderLabel': 'male',
      'time_in_space': 1,
      'date_of_death': '2263-01-01T00:00:00Z'}],) 
\end{verbatim}

\vspace{5mm}
\noindent The \texttt{datetime64[ns]} data type in pandas/NumPy is limited to the range of dates from \texttt{`1677-09-21'} to \texttt{`2262-04-11'} because it stores dates as 64-bit integers representing nanoseconds since the Unix epoch (1 January 1970). Therefore, if NumPy tries to convert the date of death, the script will crash. This shows yet another example of how the data script can be made more robust. Specifically, by checking that the dates are within range that pandas/NumPy can process.

\section{Discussion}
Our structured literature study on PBT has provided in-depth insights into this approach as an alternative to traditional testing methods. We identified the key aspects of PBT and compared the advantages and disadvantages with those of conventional methods, some of which have been addressed in existing research. Furthermore, the literature has explored the suitability of PBT across different levels of the test pyramid.

Through developing a prototype using Hypothesis for Python, we have illustrated the concepts from the literature. Hypothesis fundamentally supports the core functions of PBT. For instance, it facilitates the generation of random inputs for automated test execution while also enabling the implementation of specific data generators. This capability approximates the formal verification of essential software components.

Moreover, with the help of this prototype, Hypothesis was applied to an exemplary data analysis script at the DLR. In our case study, we successfully generated complex inputs for tests, identifying ways to make the script more robust as demonstrated in the date-range example. Besides direct analysis functions, Hypothesis enables extensive testing of the input data. It can be tailored similar to unit tests and integrated with pytest, supplementing or replacing the traditionally labor-intensive and numerous unit tests. This enhances the reliability of analysis scripts and is generally accessible even to less experienced software developers.

PBT fundamentally is a method that relies less on technical know-how and more on logical specifications of systems, thereby likely fostering collaboration between software developers and testers. However, the known drawbacks of PBT were also evident. Specifically, the increased effort required to develop specialised input generators and a potential reduction in testing efficiency should be noted.

The applicability of Hypothesis in the context of DLR is therefore conceivable. However, the scope of this work is limited as it only represents a relatively simple use case. We have demonstrated the basic functionality of Hypothesis and its application to an exemplary analysis script. In order to fully assess its suitability as a complement to established testing methods and its potential benefits for DLR, further structured and comprehensive use cases need to be investigated. This will lead to a more definitive understanding of the role and value of PBT in improving software testing practices in research-intensive environments such as the DLR.

\section{Conclusion}
In this study, we have explored and applied the method of PBT, enriching our understanding with insights from existing research and an introduction to the Hypothesis framework. It presents notable advantages, by offering capabilities for automated and randomised generation of test inputs to test general properties of systems. PBT proposes a way to supplement traditional testing methods, significantly enhancing the reliability of developed systems.

In addition, our prototype demonstrated a potential application for PBT in the context of data analysis scripts at the DLR. This practical example implies the possible benefits and utility of PBT in real-world research settings.

However, it is important to acknowledge the limitations of our study, primarily due to the narrow scope of our prototype. The results we have presented here are preliminary and need to be further validated by applying PBT to a broader range of test cases and larger-scale projects. This will enable a more comprehensive understanding of the effectiveness of PBT and its potential to transform software testing practices, particularly in complex and data-intensive research environments like those at DLR. This future work will be crucial in fully realising the benefits of PBT for enhancing software reliability and confidence in research outcomes.

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{references}
\end{document}
